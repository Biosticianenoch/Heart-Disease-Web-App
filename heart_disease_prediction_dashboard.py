# -*- coding: utf-8 -*-
"""Heart Disease Prediction Dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14boUcNoP2G--AI1DX20BwgyYrHHDJtg3

Build Web App for Heart Disease with Streamlit
"""

## Importing the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import janitor
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")
from google.colab import drive
drive.mount('/content/drive')
import time

## Load the dataset
file_path = '/content/drive/My Drive/heart.csv'
df = pd.read_csv(file_path).clean_names()

## View the first few observations of the dataset
df.head(10)

## Assess the structure of the dataset
df.info()

## Checking for missing values
df.isna().sum()

## Check for duplicates
df.duplicated().sum()

## Number of males and females whose heart data is stored in the dataset
df.sex.value_counts()

## Count of the number of males and females who have heart disease
df.sex[df.target==1].value_counts()

## Visualize your results
df.sex[df.target==1].value_counts().plot(kind='bar',figsize=(10,6),color=['green','blue'])
plt.title("Count of the number of males and females with heart disease")
plt.xticks(rotation=0);

## Contingency table
pd.crosstab(df.target,df.sex)

## Visualize the contingency table
pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=["lightblue","pink"])
plt.title("Frequency of Heart Disease vs Sex")
plt.xlabel("0= Heart Disease, 1= No disease")
plt.ylabel("Number of people with heart disease")
plt.legend(["Female","Male"])
plt.xticks(rotation=0);

## Building a Correlation Matrix
df.corr()

## Visualize the correlation matrix
cor_mat=df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(cor_mat,annot=True,cmap="coolwarm", fmt = ".2f")
plt.title("Correlation Matrix")
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.show()

"""## Machine Learning"""

## Standardization using the MinMax Scaling
from sklearn.preprocessing import MinMaxScaler
scal=MinMaxScaler()
feat=['age', 'trestbps', 'thalach' ,'oldpeak' , 'chol']
df[feat] = scal.fit_transform(df[feat])
df.head()

## Creating Features and Target variable
X=df.drop("target",axis=1).values
Y=df.target.values

## Splitting the data into train and test sets
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=42,test_size=0.2)

## Create a function for evaluating metrics
from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix

def evaluation(Y_test,Y_pred):
  acc=accuracy_score(Y_test,Y_pred)
  rcl=recall_score(Y_test,Y_pred)
  f1=f1_score(Y_test,Y_pred)


  metric_dict={'accuracy': round(acc,3),
               'recall': round(rcl,3),
               'F1 score': round(f1,3),

              }

  return print(metric_dict)

"""#### Fitting and Comparing different Models"""

## K-Nearest Neighbors
np.random.seed(42)
from sklearn.neighbors import KNeighborsClassifier
Knn_clf=  KNeighborsClassifier()
Knn_clf.fit(X_train,Y_train)
Knn_Y_pred=Knn_clf.predict(X_test)
Knn_score=Knn_clf.score(X_test,Y_test)
#print(Knn_score)
evaluation(Y_test,Knn_Y_pred)

## Logistic Regression
np.random.seed(42)
from sklearn.linear_model import LogisticRegression
LR_clf=LogisticRegression()
LR_clf.fit(X_train,Y_train)
LR_Y_pred=LR_clf.predict(X_test)
LR_score=LR_clf.score(X_test,Y_test)
#print(LR_score)
evaluation(Y_test,LR_Y_pred)

## Random Forest
np.random.seed(42)
from sklearn.ensemble import RandomForestClassifier
RF_clf=RandomForestClassifier(n_estimators=450)
RF_clf.fit(X_train,Y_train)
RF_score=RF_clf.score(X_test,Y_test)
RF_Y_pred=RF_clf.predict(X_test)
#print(RF_score)
evaluation(Y_test,RF_Y_pred)

## Support Vector Machines
np.random.seed(42)
from sklearn.svm import SVC
SVC_clf=SVC()
SVC_clf.fit(X_train,Y_train)
SVC_score=SVC_clf.score(X_test,Y_test)
SVC_Y_pred=SVC_clf.predict(X_test)
#print(SVC_score)
evaluation(Y_test,SVC_Y_pred)

## XGBOOST
from xgboost import XGBClassifier
XGB_clf=XGBClassifier()
XGB_clf.fit(X_train,Y_train)
XGB_score=XGB_clf.score(X_test,Y_test)
XGB_Y_pred=XGB_clf.predict(X_test)
#print(SVC_score)
evaluation(Y_test,XGB_Y_pred)

"""## Comparing the performance of different models"""

## Model comparison
model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',
                    'K-Nearest Neighbour','Support Vector Machine',"XGBoost"], 'Accuracy': [LR_score*100,
                    RF_score*100,Knn_score*100,SVC_score*100,XGB_score*100]})
model_comp

"""# Model Tuning"""

## Tuning KNN
neighbors = range(1, 21) # 1 to 20

# Setup algorithm
knn = KNeighborsClassifier()

# Loop through different neighbors values
for i in neighbors:
    knn.set_params(n_neighbors = i) # set neighbors value

    # Fit the algorithm
    print(f"Accuracy with {i} no. of neighbors: {knn.fit(X_train, Y_train).score(X_test,Y_test)}%")

## Fitting the KNN with the best numner of neighbors
np.random.seed(42)
from sklearn.neighbors import KNeighborsClassifier
Knn_clf=  KNeighborsClassifier(n_neighbors=1)
Knn_clf.fit(X_train,Y_train)
Knn_Y_pred=Knn_clf.predict(X_test)
Knn_score=Knn_clf.score(X_test,Y_test)
evaluation(Y_test,Knn_Y_pred)

## Hyper parameter tuning SVC using GridSearchCV
from sklearn.model_selection import GridSearchCV

# defining parameter range
param_grid = {'C': [0.1, 1,2, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf','linear']}

gs_clf = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3)

# fitting the model for grid search
gs_clf.fit(X_train, Y_train)

print(gs_clf.best_params_)

print(f"Accuracy score:{gs_clf.score(X_test,Y_test)}%")

## Hyper parameter tuning KNN using GridSearchCV
knn_grid={'n_neighbors': np.arange(1,30,1),
          'leaf_size': np.arange(1,50,1)}

gs_knn=GridSearchCV(KNeighborsClassifier(),param_grid=knn_grid,cv=5,verbose=True)

gs_knn.fit(X_train, Y_train)

gs_knn.best_params_

print(f"Accuracy score:{gs_knn.score(X_test,Y_test)*100}%")

xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27,
                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)

xgb.fit(X_train,Y_train)
xgb_score=XGB_clf.score(X_test,Y_test)
xgb_Y_pred=XGB_clf.predict(X_test)
#print(SVC_score)
evaluation(Y_test,xgb_Y_pred)

"""## Comparing Models after hyperparameter tuning"""

model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',
                    'K-Nearest Neighbour','Support Vector Machine','Extreme Gradient Boost'], 'Accuracy': [LR_score*100,
                    RF2_acc_score*100,Knn_score*100,SVC_score*100, XGB_score*100]})
model_comp

print(" Best evaluation parameters achieved with KNN:")
evaluation(Y_test, xgb_Y_pred)

"""# Visualization of the Models Results"""

final_metrics={'Accuracy': xgb.score(X_test,Y_test),
                   'Precision': precision_score(Y_test,xgb_Y_pred),
                   'Recall': recall_score(Y_test,xgb_Y_pred),
                   'F1': f1_score(Y_test,xgb_Y_pred),
                   'AUC': roc_auc_score(Y_test,xgb_Y_pred)}

metrics=pd.DataFrame(final_metrics,index=[0])

metrics.T.plot.bar(title='Final metric evaluation',legend=False);

"""# Create the confusion matrix of the best model"""

from sklearn.metrics import confusion_matrix

fig,ax=plt.subplots()
ax=sns.heatmap(confusion_matrix(Y_test,xgb_Y_pred),annot=True,cbar=True);

"""#Let's save our model using pickle"""

## Lets save our model using pickle
import pickle as pkl
pkl.dump(xgb,open("final_model.p","wb"))

"""#Import streamlit,pyngrok, and ngrok modules"""

import sklearn
sklearn_version = sklearn.__version__
print(sklearn_version)
from pyngrok import ngrok

"""#Write a file for creating web app"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile healthy-heart-app.py
# import streamlit as st
# import base64
# import sklearn
# import numpy as np
# import pickle as pkl
# from sklearn.preprocessing import MinMaxScaler
# scal=MinMaxScaler()
# #Load the saved model
# model=pkl.load(open("final_model.p","rb"))
# st.set_page_config(page_title="Heart Disease Prediction Dashboard",page_icon="⚕️",layout="centered",initial_sidebar_state="expanded")
# 
# 
# 
# %%writefile healthy-heart-app.py
# import streamlit as st
# import base64
# import sklearn
# import numpy as np
# import pickle as pkl
# from sklearn.preprocessing import MinMaxScaler
# scal=MinMaxScaler()
# #Load the saved model
# model=pkl.load(open("final_model.p","rb"))
# 
# 
# 
# 
# 
# st.set_page_config(page_title="Heart Disease Prediction Dashboard",page_icon="⚕️",layout="centered",initial_sidebar_state="expanded")
# 
# 
# 
# def preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal ):
# 
# 
#     # Pre-processing user input
#     if sex=="male":
#         sex=1
#     else: sex=0
# 
# 
#     if cp=="Typical angina":
#         cp=0
#     elif cp=="Atypical angina":
#         cp=1
#     elif cp=="Non-anginal pain":
#         cp=2
#     elif cp=="Asymptomatic":
#         cp=2
# 
#     if exang=="Yes":
#         exang=1
#     elif exang=="No":
#         exang=0
# 
#     if fbs=="Yes":
#         fbs=1
#     elif fbs=="No":
#         fbs=0
# 
#     if slope=="Upsloping: better heart rate with excercise(uncommon)":
#         slope=0
#     elif slope=="Flatsloping: minimal change(typical healthy heart)":
#           slope=1
#     elif slope=="Downsloping: signs of unhealthy heart":
#         slope=2
# 
#     if thal=="fixed defect: used to be defect but ok now":
#         thal=6
#     elif thal=="reversable defect: no proper blood movement when excercising":
#         thal=7
#     elif thal=="normal":
#         thal=2.31
# 
#     if restecg=="Nothing to note":
#         restecg=0
#     elif restecg=="ST-T Wave abnormality":
#         restecg=1
#     elif restecg=="Possible or definite left ventricular hypertrophy":
#         restecg=2
# 
# 
#     user_input=[age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal]
#     user_input=np.array(user_input)
#     user_input=user_input.reshape(1,-1)
#     user_input=scal.fit_transform(user_input)
#     prediction = model.predict(user_input)
# 
#     return prediction
# 
# 
# 
# 
#     # front end elements of the web page
# html_temp = """
#     <div style ="background-color:pink;padding:13px">
#     <h1 style ="color:black;text-align:center;">Heart Disease Prediction Dashboard</h1>
#     </div>
#     """
# 
# # display the front end aspect
# st.markdown(html_temp, unsafe_allow_html = True)
# st.subheader('by Enock Bereka')
# 
# # following lines create boxes in which user can enter data required to make prediction
# age=st.selectbox ("Age",range(1,121,1))
# sex = st.radio("Select Gender: ", ('male', 'female'))
# cp = st.selectbox('Chest Pain Type',("Typical angina","Atypical angina","Non-anginal pain","Asymptomatic"))
# trestbps=st.selectbox('Resting Blood Sugar',range(1,500,1))
# restecg=st.selectbox('Resting Electrocardiographic Results',("Nothing to note","ST-T Wave abnormality","Possible or definite left ventricular hypertrophy"))
# chol=st.selectbox('Serum Cholestoral in mg/dl',range(1,1000,1))
# fbs=st.radio("Fasting Blood Sugar higher than 120 mg/dl", ['Yes','No'])
# thalach=st.selectbox('Maximum Heart Rate Achieved',range(1,300,1))
# exang=st.selectbox('Exercise Induced Angina',["Yes","No"])
# oldpeak=st.number_input('Oldpeak')
# slope = st.selectbox('Heart Rate Slope',("Upsloping: better heart rate with excercise(uncommon)","Flatsloping: minimal change(typical healthy heart)","Downsloping: signs of unhealthy heart"))
# ca=st.selectbox('Number of Major Vessels Colored by Flourosopy',range(0,5,1))
# thal=st.selectbox('Thalium Stress Result',range(1,8,1))
# 
# 
# 
# #user_input=preprocess(sex,cp,exang, fbs, slope, thal )
# pred=preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal)
# 
# 
# 
# 
# if st.button("Predict"):
#   if pred[0] == 0:
#     st.error('Warning! You have high risk of getting a heart attack!')
# 
#   else:
#     st.success('You have lower risk of getting a heart disease!')
# 
# 
# 
# 
# 
# st.sidebar.subheader("About App")
# 
# st.sidebar.info("This web app is helps you to find out whether you are at a risk of developing a heart disease.")
# st.sidebar.info("Enter the required fields and click on the 'Predict' button to check whether you have a healthy heart")
# st.sidebar.info("Don't forget to rate this app")
# 
# 
# 
# feedback = st.sidebar.slider('How much would you rate this app?',min_value=0,max_value=5,step=1)
# 
# if feedback:
#   st.header("Thank you for rating the app!")
#   st.info("Caution: This is just a prediction and not doctoral advice. Kindly see a doctor if you feel the symptoms persist.")
# 
# 
# 
#
